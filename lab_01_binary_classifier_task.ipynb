{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7f3a738-486b-45d6-8f18-0e4cda691378",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Machine Learning in a Nutshell: Binary Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fb85a-8672-4242-8f02-2f1c99ddbe5d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## A (given) Binary Classifier\n",
    "\n",
    "To begin, we assume the weights of a binary classifier are given as $w_1 = 1.0$ and $w_2 = 0.5$.\n",
    "In the second task we are going to learn the weights from data.\n",
    "\n",
    "We also assume, the feature extractor $\\phi(\\mathbf{x})$ is given as the identity function. For simplicity, we are working with the extracted feature vectors $[x_1, x_2]$ directly.\n",
    "\n",
    "### Our classifier\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f_\\mathbf{w}(x) &= \\text{sign}(\\underbrace{[1, 0.5]}_{\\mathbf{w}} \\cdot \\underbrace{[x_1, x_2]}_{\\phi(x)})\\\\\n",
    "      &= \\text{sign}(\\underbrace{1}_{w_1} \\cdot x_1 + \\underbrace{0.5}_{w_2} \\cdot x_2)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "### The sign-function maps from a score to a label\n",
    "$$\n",
    "\\begin{equation*}\n",
    "  \\text{sign}(z)=\\begin{cases}\n",
    "    +1, & \\text{if $z>0$}\\\\\n",
    "    -1, & \\text{if $z<0$}\\\\\n",
    "     0, & \\text{if $z = 0$}.\n",
    "  \\end{cases}\n",
    "\\end{equation*}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7d5406",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1: Making predictions with the classifier $f$\n",
    "\n",
    "In order to make predictions, we need to implement $f$ in Python. This requires us to implement the individual pieces as python methods:\n",
    "- A `getScore` method that computes the **score**: $w_1 \\cdot x_1 + w_2 \\cdot x_2$\n",
    "- A `getLabel` method that computes the **label** from a given **score**: $\\text{sign}(\\cdot)$\n",
    "- A `predict` method that computes the **label** from two parameters $x_1$ and $x_2$ by calling the other methods.\n",
    "\n",
    "Define a class and implement the `getScore`, `getLabel` and `predict` methods.\n",
    "The constructor should expect two parameters $w_0$ and $w_1$ and store the values in instance variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e033369a",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, -2) -> predicted label: 1 target label: 1 Margin: 1.0, HingeLoss: 0\n",
      "Predicted label: 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class BinaryClassifier:\n",
    "\n",
    "    def __init__(self, w1, w2):\n",
    "        self.w1 = w1\n",
    "        self.w2 = w2\n",
    "\n",
    "    def getScore(self, x1, x2):\n",
    "        z = w1*x1 + w2*x2\n",
    "        return z\n",
    "\n",
    "    def getLabel(self, z):\n",
    "        if z>0:\n",
    "            label = 1\n",
    "        elif z<0: \n",
    "            label = -1\n",
    "        else:\n",
    "            label = 0\n",
    "        return label    \n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "    def predict(self, x1, x2): \n",
    "        score = self.getScore(x1, x2)\n",
    "        return self.getLabel(score)\n",
    "\n",
    "    def getMargin(self, x1, x2, y):\n",
    "        margin = self.getScore(x1, x2)*y\n",
    "        return margin\n",
    "\n",
    "    def getHingeLoss(self, x1, x2, y, minMargin=1.0):\n",
    "        # gap = minMargin\n",
    "        score = self.getScore(x1, x2)\n",
    "        gap = minMargin -score*y\n",
    "        loss = max(0, gap)\n",
    "        return loss\n",
    "\n",
    "    def verbosePrediction(self, x1, x2, y):\n",
    "        # print(f\"({x1}, {x2}) -> predicted label: {self.predict(x1, x2)} target label: {y} Margin: {self.getMargin(x1, x2, y)} HingeLoss: {self.getHingeLoss(x1, x2, y)}\")\n",
    "        print(f\"({x1}, {x2}) -> predicted label: {self.predict(x1, x2)} target label: {y} Margin: {self.getMargin(x1, x2, y)}, HingeLoss: {self.getHingeLoss(x1, x2, y)}\") \n",
    "\n",
    "    def train(self, x1, x2, y, learningRate=0.1):\n",
    "        # Training with a weight update for a single point\n",
    "        loss = self.getHingeLoss(x1, x2, y)\n",
    "        if loss > 0:\n",
    "            self.w1 += learningRate * y * x1\n",
    "            self.w2 += learningRate * y * x2\n",
    "\n",
    "    def train_batch(self, points, ys, learningRate=0.1):\n",
    "        grad_w1 = 0\n",
    "        grad_w2 = 0\n",
    "\n",
    "        # Number of points in the batch\n",
    "        n = len(points)\n",
    "\n",
    "        # Loop through all points in the batch\n",
    "        for i in range(n):\n",
    "            x1, x2 = points[i]\n",
    "            y = ys[i]\n",
    "            \n",
    "            # Compute hinge loss for the point\n",
    "            loss = self.getHingeLoss(x1, x2, y)\n",
    "            \n",
    "            # If loss > 0, calculate gradients\n",
    "            if loss > 0:\n",
    "                grad_w1 += y * x1\n",
    "                grad_w2 += y * x2\n",
    "\n",
    "        # Average gradients\n",
    "        grad_w1 /= n\n",
    "        grad_w2 /= n\n",
    "\n",
    "        # Update weights\n",
    "        self.w1 += learningRate * grad_w1\n",
    "        self.w2 += learningRate * grad_w2\n",
    "\n",
    "w1 = 1.0\n",
    "w2 = 0.5\n",
    "\n",
    "x1 = 2\n",
    "x2 = -2\n",
    "y=1\n",
    "\n",
    "classifier = BinaryClassifier(w1, w2)\n",
    "classifier.train(x1, x2, y, learningRate=0.1)\n",
    "classifier.verbosePrediction(x1, x2, y)\n",
    "predicted_label = classifier.predict(x1, x2)\n",
    "print(f\"Predicted label: {predicted_label}\")\n",
    "classifier.predict(x1, x2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad3f388-ba94-447e-abd4-7b517a246714",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebe01b7",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Multiple predictions and the target labels\n",
    "\n",
    "Below you are given a list of training examples.\n",
    "Each training example comes with a target label.\n",
    "\n",
    "The target label is the label our classifier should predict, ... but most likely it doesn't since we did not train it yet.\n",
    "\n",
    "Let's first make the classifier predict a label for each training example and print the predicted label next to the target label such we can see when things go wrong.\n",
    "\n",
    "#### Implement printing as a method of `BinaryClassifier`, named `verbosePrediction` that expects $x_1$, $x_2$ and $y$ as parameters and prints, for each training example, the predicted label and the target label and if it is correct."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2080091",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "((0.5, 0.5)) -> predicted label: 1.0 target label: 1 Margin: 0.75, HingeLoss: 0.25\n",
      "((2, 0)) -> predicted label: 1.0 target label: 1 Margin: 2.0, HingeLoss: 0\n",
      "((-1, 1)) -> predicted label: -1.0 target label: 1 Margin: -0.5, HingeLoss: 1.5\n",
      "((1, -1)) -> predicted label: 1.0 target label: -1 Margin: -0.5, HingeLoss: 1.5\n",
      "((1, -2)) -> predicted label: 0.0 target label: -1 Margin: -0.0, HingeLoss: 1.0\n",
      "((-1, -1)) -> predicted label: -1.0 target label: -1 Margin: 1.5, HingeLoss: 0\n"
     ]
    }
   ],
   "source": [
    "data = [(0.5, 0.5), (2, 0), (-1, 1), (1, -1), (1, -2), (-1, -1)]\n",
    "labels = [1, 1, 1, -1, -1, -1]\n",
    "\n",
    "for x, y in zip(data, labels):\n",
    "    # commented out, because it won't work right from the start. you have to implement the methods first in BinaryClassifier\n",
    "    # print(classifier.predict(x[0], x[1]) == y)\n",
    "    # classifier.predict(x[0], x[1])\n",
    "    # classifier.verbosePrediction(x[0], x[1], y)\n",
    "    classifier.verbosePrediction(x, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701b9b44",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Compute the margin\n",
    "\n",
    "Recall, the **score** of a training example is positive if the angle between the feature vector of the training example and the weight vector is acute. And the score is negative if the angle is obtuse.\n",
    "$$\n",
    "\\mathbf{w} \\cdot \\mathbf{x} = \\Vert \\mathbf{w} \\Vert \\cdot \\Vert \\mathbf{x} \\Vert \\cdot \\cos(\\omega)\n",
    "$$\n",
    "\n",
    "All points in feature space with a score = 0, constitute the decision boundary of the classifier.\n",
    "\n",
    "Now, the **margin** takes the score and the target label and quantifies the correctness of a prediction:\n",
    "$$\n",
    "\\text{margin}(\\mathbf{x}, y, \\mathbf{w}) = (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y\n",
    "$$\n",
    "\n",
    "The margin is positive if the score and the target label agree (regardless of which class it is). It is negative, if both quantities disagree.\n",
    "\n",
    "Add a method `getMargin` to your implementation and, additionally, print the margin for each training example in the `predictVerbose` method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830159d2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "193cdef6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Compute the Hinge-Loss of the classifier\n",
    "\n",
    "Based on the margin, we can now calculate the Hinge-Loss of the classifier.\n",
    "\n",
    "Like any other loss, the Hinge Loss is large if the classifier's prediction does **not** agree with the target label.\n",
    "This is the exact opposite of what the margin tells us.\n",
    "\n",
    "Thus, we can just use the negative margin to indicate disagreement between prediction and target label.\n",
    "\n",
    "$$\n",
    "\\text{Loss}_{hinge}(x, y, \\mathbf{w}) = \\max \\{ \\text{gap} - \\underbrace{\\underbrace{(\\mathbf{w} \\cdot \\mathbf{x})}_{\\text{score}}y}_{\\text{margin}}, 0 \\}\n",
    "$$\n",
    "\n",
    "Intuitively, what is the role of $\\text{gap}$?\n",
    "- What if $gap = 0$?\n",
    "- What if $gap > 0$?\n",
    "\n",
    "Hint: Think about the closeness of training points to the decision boundary (Or rather the closeness of the decision boundary to the training points, since it's up to us to adjust the decision boundary based on the loss).\n",
    "![](linear-classifier-decision-boundary.png)\n",
    "\n",
    "Implement a method `getHingeLoss` that expects parameters $x_1$, $x_2$ and, additionally, a keyword parameter `gap` with a default value of $1.0$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b643bc",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Implement the weight update\n",
    "\n",
    "Based on the hinge loss, we can construct a weight update rule and repeatedly update the weights.\n",
    "\n",
    "The goal of repeated weight updates is to adjust the decision boundary of the classifier, such that it classifies the training examples correctly.\n",
    "\n",
    "For each training example, we calculate the weight change $\\Delta \\mathbf{w}$ and then update the weights using $\\Delta \\mathbf{w}$. The update rule is given as:\n",
    "\n",
    "$$\n",
    "\\mathbf{w} = \\mathbf{w} - 0.1 \\cdot \\Delta \\mathbf{w}\n",
    "$$\n",
    "\n",
    "where the weight change is given by:\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\Delta \\mathbf{w}  = \\begin{cases}\n",
    "    - [x_1 \\cdot y, x_2 \\cdot y], & \\qquad \\text{if} \\quad \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w}) > 0\\\\\n",
    "    0, & \\qquad \\text{otherwise}.\n",
    "  \\end{cases}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note: We obtain the weight change by calculating the partial derivatives of the loss with respect to each weight. Example for $w_1$:\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial \\text{Loss}_{hinge}(\\mathbf{x}, y, \\mathbf{w})}{\\partial w_1} &= \\frac{\\partial}{\\partial w_1} (\\text{gap} - (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= \\frac{\\partial}{\\partial w_1} \\text{gap} - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y)\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot \\frac{\\partial}{\\partial (w_1 \\cdot x_1 + w_2 \\cdot x_2)} (w_1 \\cdot x_1 + w_2 \\cdot x_2) \\cdot y \\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot + w_2 \\cdot x_2)  \\cdot y\\\\\n",
    "   &= 0 - \\frac{\\partial}{\\partial w_1} (w_1 \\cdot x_1 \\cdot y)  \\cdot y\\\\\n",
    "   &= - x_1 \\cdot y\\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "The derivates for $w_2$ are computed analogously.\n",
    "The weight change $\\Delta \\mathbf{w}$ is just the vector of these partial derivatives.\n",
    "\n",
    "#### Implement the update rule in a method `train`. This method expects $x_1$, $x_2$ and $y$ as parameters. You may also include an optional `eta` parameter for the learning rate, which is currently set to a fixed 0.1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dbac805",
   "metadata": {},
   "source": [
    "### Task 6: Implement Stochastic Gradient Descent in the method `train_batch`.\n",
    "Computing updates on each training example is computationally expensive and does not yield a good sample of our ground truth. \n",
    "Instead, we should compute the updates on multiple training examples as their average.\n",
    "This also greatly reduces the variance of the updates and should lead to a more stable convergence.\n",
    "All you need to do is compute the average of the weight changes for each training example and update the weights accordingly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "602bb4a0",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Congratulations! You have implemented the core machinery of machine learning algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2212b8",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
